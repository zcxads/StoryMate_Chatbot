from typing import Dict, Any
from langsmith import traceable
from app.core.llm.prompt_manager import PromptManager
from app.core.llm.llm_provider import LLMProvider
from app.logs.logger import setup_logger
from app.utils.language_detector import detect_language
from app.config import settings

logger = setup_logger('answer_generator_agent')


class AnswerGeneratorAgent:
    """ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì—ì´ì „íŠ¸"""
    
    def __init__(self, rag_system):
        """ë‹µë³€ ìƒì„± ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        self.rag_system = rag_system
        self.prompt_manager = PromptManager()
    
    async def generate_answer(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ë©”ëª¨ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            query = state["query"]
            intent = state.get("intent", "general_chat")
            retrieved_documents = state.get("retrieved_documents", []) or []
            character_genre = state.get("character_genre", None)  # ìºë¦­í„° ì¥ë¥´ ê°€ì ¸ì˜¤ê¸°

            # ì–¸ì–´ ìë™ ê°ì§€
            detected_language = detect_language(query)
            logger.info(f"ğŸŒ ê°ì§€ëœ ì–¸ì–´: {detected_language} - ì¿¼ë¦¬: {query[:50]}...")

            # ìºë¦­í„° ì¥ë¥´ ë¡œê·¸
            if character_genre:
                logger.info(f"ğŸ­ ìºë¦­í„° ì¥ë¥´: {character_genre}")

            # LLM ê°€ì ¸ì˜¤ê¸°
            llm_provider = LLMProvider()
            llm = llm_provider.get_or_create_llm(settings.ANSWER_GENERATION_MODEL)

            logger.info(f" ì˜ë„ë³„ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {intent}")

            # ì¼ë°˜ ì±„íŒ…: ë¬¸ì„œ ì—†ì´ ë°”ë¡œ ë‹µë³€ ìƒì„±
            if intent == "general_chat":
                logger.info("ğŸ’¬ ì¼ë°˜ ì±„íŒ… ëª¨ë“œ: ë¬¸ì„œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ì—†ì´ ì§ì ‘ ë‹µë³€ ìƒì„±")

                # ì¼ë°˜ ì±„íŒ…ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                system_prompt = self.prompt_manager.get_prompt(
                    "multiturn_answer_generation",
                    "answer_generation.system_prompt",
                    language=detected_language
                )

                # ìºë¦­í„° ì¥ë¥´ë³„ ë§íˆ¬ ì¶”ê°€
                character_tone = self.prompt_manager.get_character_tone_instruction(character_genre)
                if character_tone:
                    system_prompt = f"{system_prompt}\n\n{character_tone}"

                prompt_template = self.prompt_manager.get_prompt(
                    "multiturn_answer_generation",
                    "answer_generation.general_chat_prompt",
                    language=detected_language
                )
                task_prompt = prompt_template.format(query=query)
                prompt = f"{system_prompt}\n\n{task_prompt}"

                # LangSmith ì¶”ì ìš© ì»¨í…ìŠ¤íŠ¸ ê¸°ë¡
                context_info = {
                    "system_prompt": system_prompt,
                    "task_prompt": task_prompt,
                    "retrieved_documents": "",
                    "conversation_history": "",
                    "detected_language": detected_language
                }
                await self._log_final_prompt(prompt, intent, context_info)

            else:
                retrieved_docs_parts = [doc.page_content for doc in retrieved_documents if getattr(doc, "page_content", "")]
                conversation_parts = []
                conversation_history = state.get("conversation_history") or []
                
                # í´ë°± í”Œë˜ê·¸ í™•ì¸ (ëŒ€í™” ê¸°ë¡ ìš°ì„  ì‚¬ìš© ì—¬ë¶€)
                fallback_to_memory = state.get("fallback_to_memory", False)
                memory_conversations = state.get("memory_conversations", [])
                
                # í´ë°± ëª¨ë“œ: ëŒ€í™” ê¸°ë¡ ìš°ì„  ì‚¬ìš©
                if fallback_to_memory and memory_conversations:
                    logger.info("í´ë°± ëª¨ë“œ: ëŒ€í™” ê¸°ë¡ ìš°ì„  ì‚¬ìš©")
                    # ëŒ€í™” ê¸°ë¡ì—ì„œ context ì¶”ì¶œ
                    memory_parts = []
                    for doc in memory_conversations[:5]:
                        if hasattr(doc, 'metadata'):
                            user_query = doc.metadata.get("query", "")
                            assistant_response = doc.metadata.get("response", "")
                            if user_query and assistant_response:
                                memory_parts.append(f"Q: {user_query}\nA: {assistant_response}\n")
                    
                    # ì‹¤ì œ ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ì ìœ¼ë©´ ëŒ€í™” ê¸°ë¡ì„ ì£¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
                    if len(retrieved_docs_parts) < 3:
                        retrieved_docs_parts = ["\n".join(memory_parts)] if memory_parts else retrieved_docs_parts
                        logger.info(f"ëŒ€í™” ê¸°ë¡ì„ ì£¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš© ({len(memory_parts)}ê°œ ëŒ€í™”)")
                    else:
                        # ì‹¤ì œ ë¬¸ì„œì™€ ëŒ€í™” ê¸°ë¡ ë³‘í•©
                        conversation_parts.extend(memory_parts)
                        logger.info(f"ì‹¤ì œ ë¬¸ì„œ + ëŒ€í™” ê¸°ë¡ ë³‘í•© (ë¬¸ì„œ: {len(retrieved_docs_parts)}ê°œ, ëŒ€í™”: {len(memory_parts)}ê°œ)")
                
                # ì¼ë°˜ì ì¸ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
                if conversation_history:
                    for conv in conversation_history:
                        user_text = conv.get("user", "")
                        assistant_text = conv.get("assistant", "")
                        if user_text and assistant_text:
                            conversation_parts.append(f"Q: {user_text}\nA: {assistant_text}\n")
                
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì‘ë‹µ ìƒì„±
                if not retrieved_docs_parts:
                    logger.info("ğŸ“ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.")
                    answer_text = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    state["answer"] = answer_text
                    
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ë„ ëŒ€í™” ê¸°ë¡ì€ ì €ì¥í•´ì•¼ í•¨
                    try:
                        user_id = state["user_id"]
                        logger.info(f"ğŸ’¾ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì‹œì‘ (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ) - ì‚¬ìš©ì: {user_id}")
                        logger.info(f"ğŸ’¾ ì €ì¥í•  ì§ˆë¬¸: {query[:50]}... ({len(query)} ë¬¸ì)")
                        logger.info(f"ğŸ’¾ ì €ì¥í•  ë‹µë³€: {answer_text[:50]}... ({len(answer_text)} ë¬¸ì)")
                        
                        self.rag_system.chat_history_manager.add_to_chat_history(
                            user_id, query, answer_text
                        )
                        
                        logger.info("âœ… ëŒ€í™” ê¸°ë¡ ì €ì¥ ì™„ë£Œ (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)")
                    except Exception as save_error:
                        logger.error(f"âŒ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨ (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ): {save_error}", exc_info=True)
                    
                    return state

                # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                system_prompt = self.prompt_manager.get_prompt(
                    "multiturn_answer_generation",
                    "answer_generation.system_prompt",
                    language=detected_language
                )

                # ìºë¦­í„° ì¥ë¥´ë³„ ë§íˆ¬ ì¶”ê°€
                character_tone = self.prompt_manager.get_character_tone_instruction(character_genre)
                if character_tone:
                    system_prompt = f"{system_prompt}\n\n{character_tone}"

                if intent == "document_list":
                    prompt_template = self.prompt_manager.get_prompt(
                        "multiturn_answer_generation",
                        "answer_generation.document_list_prompt",
                        language=detected_language
                    )
                    task_prompt = prompt_template.format(
                        retrieved_documents="\n\n".join(retrieved_docs_parts),
                        query=query
                    )
                elif intent == "follow_up_summary":
                    # ê³¼ê±° ëŒ€í™” ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸
                    prompt_template = self.prompt_manager.get_prompt(
                        "multiturn_answer_generation",
                        "answer_generation.follow_up_summary_prompt",
                        language=detected_language
                    )

                    # reference_indexë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ëŒ€í™” ê°€ì ¸ì˜¤ê¸°
                    reference_index = state.get("reference_index")
                    reference_type = state.get("reference_type")

                    referenced_conversation = ""
                    if reference_index is not None and conversation_history:
                        try:
                            # reference_indexë¡œ í•´ë‹¹ ëŒ€í™” ê°€ì ¸ì˜¤ê¸° (0=ì²«ë²ˆì§¸, -1=ë§ˆì§€ë§‰)
                            if -len(conversation_history) <= reference_index < len(conversation_history):
                                conv = conversation_history[reference_index]
                                user_text = conv.get("user", "")
                                assistant_text = conv.get("assistant", "")
                                if user_text and assistant_text:
                                    referenced_conversation = f"Q: {user_text}\nA: {assistant_text}"
                                    logger.info(f"ğŸ“ ì°¸ì¡°ëœ ëŒ€í™”: {user_text[:50]}...")
                            else:
                                logger.warning(f"âš ï¸ reference_index={reference_index}ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨ (ëŒ€í™” ìˆ˜: {len(conversation_history)})")
                        except Exception as e:
                            logger.error(f"âŒ reference_index ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

                    # referenced_conversationì´ ì—†ìœ¼ë©´ ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚¬ìš©
                    if not referenced_conversation and conversation_parts:
                        referenced_conversation = "\n\n".join(conversation_parts)
                        logger.info(f"ğŸ” follow_up_summary - ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚¬ìš© ({len(conversation_parts)}ê°œ)")

                    if not referenced_conversation:
                        referenced_conversation = "ì´ì „ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤."
                        logger.warning("âš ï¸ ì°¸ì¡°í•  ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")

                    task_prompt = prompt_template.format(
                        conversation_history=referenced_conversation,
                        query=query
                    )
                else:
                    prompt_template = self.prompt_manager.get_prompt(
                        "multiturn_answer_generation",
                        "answer_generation.detailed_question_prompt",
                        language=detected_language
                    )
                    task_prompt = prompt_template.format(
                        retrieved_documents="\n\n".join(retrieved_docs_parts),
                        conversation_history="\n\n".join(conversation_parts) if conversation_parts else "ì´ì „ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        query=query
                    )

                prompt = f"{system_prompt}\n\n{task_prompt}"

                context_info = {
                    "system_prompt": system_prompt,
                    "task_prompt": task_prompt,
                    "retrieved_documents": "\n\n".join(retrieved_docs_parts),
                    "conversation_history": "\n\n".join(conversation_parts) if conversation_parts else "",
                    "detected_language": detected_language
                }
                await self._log_final_prompt(prompt, intent, context_info)
            
            response = llm.invoke(prompt)
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê°œì„ 
            answer_text = None
            
            # LangChain ChatResult/ChatGeneration ì²˜ë¦¬
            if hasattr(response, 'content') and response.content:
                answer_text = str(response.content).strip()
                logger.info("âœ… ì‘ë‹µ ì¶”ì¶œ: response.content ì‚¬ìš©")
            elif hasattr(response, 'text') and response.text:
                answer_text = str(response.text).strip()
                logger.info("âœ… ì‘ë‹µ ì¶”ì¶œ: response.text ì‚¬ìš©")
            elif isinstance(response, str) and response.strip():
                answer_text = response.strip()
                logger.info("âœ… ì‘ë‹µ ì¶”ì¶œ: ì§ì ‘ ë¬¸ìì—´ ì‚¬ìš©")
            # Gemini íŠ¹í™” ì‘ë‹µ ì²˜ë¦¬
            elif hasattr(response, 'candidates') and response.candidates:
                try:
                    answer_text = response.candidates[0].content.parts[0].text.strip()
                    logger.info("âœ… ì‘ë‹µ ì¶”ì¶œ: Gemini candidates ì‚¬ìš©")
                except Exception as e:
                    logger.error(f"âŒ Gemini candidates ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            else:
                # ìµœí›„ ìˆ˜ë‹¨: str() ë³€í™˜í•˜ë˜ í† í°í™” ë°©ì§€
                raw_response = str(response)
                logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ í˜•ì‹: {type(response)}")
                logger.info(f"ğŸ” Raw response preview: {raw_response[:200]}...")
                
                # í† í°í™”ëœ ì‘ë‹µì¸ì§€ í™•ì¸ (ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ëœ í† í°ë“¤)
                if " " in raw_response and len(raw_response.split()) > 100:
                    logger.error("âŒ í† í°í™”ëœ ì‘ë‹µ ê°ì§€ë¨ - ì›ë³¸ í…ìŠ¤íŠ¸ ë³µì› ì‹œë„")
                    # í† í°ì„ í•©ì³ì„œ ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ë³µì› ì‹œë„
                    answer_text = raw_response.replace(" ", "")
                else:
                    answer_text = raw_response
            
            # ìµœì¢… ê²€ì¦
            if not answer_text or len(answer_text.strip()) == 0:
                answer_text = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                logger.error("âŒ ë¹ˆ ì‘ë‹µ - ê¸°ë³¸ ë©”ì‹œì§€ë¡œ ëŒ€ì²´")
            
            logger.info(f"ğŸ¯ ìµœì¢… ì‘ë‹µ ê¸¸ì´: {len(answer_text)} ë¬¸ì")
            logger.info(f"ğŸ¯ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {answer_text[:100]}...")
            
            state["answer"] = answer_text
            
            # ëŒ€í™” ê¸°ë¡ ì €ì¥
            try:
                user_id = state["user_id"]
                logger.info(f"ğŸ’¾ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì‹œì‘ - ì‚¬ìš©ì: {user_id}")
                logger.info(f"ğŸ’¾ ì €ì¥í•  ì§ˆë¬¸ ê¸¸ì´: {len(query)} ë¬¸ì")
                logger.info(f"ğŸ’¾ ì €ì¥í•  ë‹µë³€ ê¸¸ì´: {len(answer_text)} ë¬¸ì")
                
                self.rag_system.chat_history_manager.add_to_chat_history(
                    user_id, query, answer_text
                )
                
                logger.info("âœ… ëŒ€í™” ê¸°ë¡ ì €ì¥ ì™„ë£Œ")
            except Exception as save_error:
                logger.error(f"âŒ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {save_error}", exc_info=True)
            
            logger.info("ğŸ¯ ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            state["error"] = f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}"
        
        return state
    
    @traceable(name="Final Prompt Construction", run_type="tool")
    async def _log_final_prompt(self, prompt: str, intent: str, context_info: Dict) -> Dict:
        """ìµœì¢… í”„ë¡¬í”„íŠ¸ ì •ë³´ë¥¼ LangSmithì— ê¸°ë¡í•˜ê³  ì½˜ì†”ì— ë¯¸ë¦¬ë³´ê¸°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""

        system_prompt = context_info.get("system_prompt") or ""
        task_prompt = context_info.get("task_prompt") or ""
        retrieved_documents = context_info.get("retrieved_documents") or ""
        conversation_history = context_info.get("conversation_history") or ""
        detected_language = context_info.get("detected_language", "unknown")

        # í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸° ë¡œê·¸
        logger.info(f"ğŸ“ ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Intent: {intent}, Language: {detected_language})")
        logger.info(f"  ì´ ê¸¸ì´: {len(prompt)} ë¬¸ì")

        # ì„¹ì…˜ë³„ ê¸¸ì´ ì •ë³´
        logger.info(f"  ì„¹ì…˜ë³„ êµ¬ì„±:")
        logger.info(f"    - System Prompt: {len(system_prompt)} ë¬¸ì")
        logger.info(f"    - Task Prompt: {len(task_prompt)} ë¬¸ì")
        logger.info(f"    - Retrieved Documents: {len(retrieved_documents)} ë¬¸ì")
        logger.info(f"    - Conversation History: {len(conversation_history)} ë¬¸ì")

        logger.info(f"  í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°:\n{prompt[:200]}")

        prompt_analysis = {
            "intent": intent,
            "final_prompt": prompt,
            "prompt_sections": {
                "system_prompt": system_prompt,
                "task_prompt": task_prompt,
                "retrieved_documents": retrieved_documents,
                "conversation_history": conversation_history
            }
        }

        return prompt_analysis
