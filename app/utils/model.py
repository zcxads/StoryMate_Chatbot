from typing import Optional, List, Any, Dict, Union
from dataclasses import dataclass

from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.outputs import ChatGeneration, ChatResult

import anthropic
import google.generativeai as genai

from app.logs.logger import setup_logger
from app.config import settings

logger = setup_logger('model')


# ìƒìˆ˜ ì •ì˜ (configì—ì„œ ê°€ì ¸ì˜´)
@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì •ì„ ìœ„í•œ ë°ì´í„° í´ë˜ìŠ¤"""
    MAX_TOKENS: int = settings.MODEL_MAX_TOKENS
    REQUEST_TIMEOUT: int = settings.MODEL_REQUEST_TIMEOUT


# ëª¨ë¸ ì´ë¦„ ë§¤í•‘
GEMINI_MODEL_MAPPING = {
    "gemini-2.5-pro": "gemini-2.5-pro-preview-05-06",
    "gemini-2.5-flash": "gemini-2.5-flash-preview-04-17",
    "gemini-2.0-flash": "gemini-2.0-flash-exp"
}

CLAUDE_MODEL_MAPPING = {
    "claude-opus-4-0": "claude-opus-4-20250514",
    "claude-sonnet-4-0": "claude-sonnet-4-20250514"
}

SUPPORTED_MODELS = {
    "openai": ["gpt-4o", "gpt-4o-mini"],
    "gemini": ["gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.0-flash"],
    "claude": ["claude-sonnet-4-0", "claude-opus-4-0"]
}


class ModelError(Exception):
    """ëª¨ë¸ ê´€ë ¨ ì˜ˆì™¸ í´ë˜ìŠ¤"""
    pass


class GeminiChatModel(BaseChatModel):
    """Google Gemini ëª¨ë¸ì„ ìœ„í•œ LangChain í˜¸í™˜ ë˜í¼"""
    
    def __init__(self, model_name: str, temperature: float = settings.TEMPERATURE, api_key: str = settings.GEMINI_API_KEY):
        super().__init__()
        
        if not api_key:
            raise ModelError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # __dict__ë¥¼ ì‚¬ìš©í•˜ì—¬ í•„ë“œ ì„¤ì • (Pydantic ì œì•½ ìš°íšŒ)
        self.__dict__['model_name'] = model_name
        self.__dict__['temperature'] = temperature
        self.__dict__['api_key'] = api_key
        
        # Gemini ì„¤ì •
        genai.configure(api_key=self.api_key)
        self.__dict__['model'] = genai.GenerativeModel(model_name)
    
    def _convert_messages_to_prompt(self, messages: List[Any]) -> str:
        """LangChain ë©”ì‹œì§€ë¥¼ Gemini í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        prompt_parts = []
        
        for message in messages:
            if hasattr(message, 'content'):
                if isinstance(message, SystemMessage):
                    prompt_parts.append(f"System: {message.content}")
                elif isinstance(message, HumanMessage):
                    prompt_parts.append(f"Human: {message.content}")
                elif isinstance(message, AIMessage):
                    prompt_parts.append(f"Assistant: {message.content}")
                else:
                    prompt_parts.append(str(message.content))
            else:
                prompt_parts.append(str(message))
        
        return "\n".join(prompt_parts)
    
    def _generate_response(self, messages: List[Any]) -> str:
        """Gemini ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            prompt = self._convert_messages_to_prompt(messages)
            
            response = self.model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=self.temperature,
                    max_output_tokens=ModelConfig.MAX_TOKENS
                )
            )
            
            return response.text
            
        except Exception as e:
            logger.error(f"Gemini ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise ModelError(f"Gemini ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _generate(self, messages: List[Any], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs) -> ChatResult:
        """LangChain í˜¸í™˜ ë©”ì‹œì§€ ìƒì„± ë©”ì„œë“œ"""
        try:
            response_text = self._generate_response(messages)
            
            return ChatResult(
                generations=[
                    ChatGeneration(
                        message=AIMessage(content=response_text)
                    )
                ]
            )
            
        except Exception as e:
            logger.error(f"Gemini ë©”ì‹œì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    @property
    def _llm_type(self) -> str:
        return "gemini"
    
    def invoke(self, input_data, config: Optional[Dict[str, Any]] = None, **kwargs) -> AIMessage:
        """LangChain 0.2.x í˜¸í™˜ invoke ë©”ì„œë“œ - ë¬¸ìì—´ê³¼ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ëª¨ë‘ ì§€ì›"""
        try:
            # ë¬¸ìì—´ ì…ë ¥ ì²˜ë¦¬
            if isinstance(input_data, str):
                logger.info("ğŸ” ë¬¸ìì—´ í”„ë¡¬í”„íŠ¸ ì§ì ‘ ì²˜ë¦¬")
                response = self.model.generate_content(
                    input_data,
                    generation_config=genai.types.GenerationConfig(
                        temperature=self.temperature,
                        max_output_tokens=ModelConfig.MAX_TOKENS
                    )
                )
                
                # ì‘ë‹µ í…ìŠ¤íŠ¸ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
                response_text = ""
                if hasattr(response, 'text') and response.text:
                    response_text = response.text.strip()
                elif hasattr(response, 'candidates') and response.candidates:
                    try:
                        response_text = response.candidates[0].content.parts[0].text.strip()
                    except Exception as e:
                        logger.error(f"âŒ Gemini candidates ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        response_text = "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                
                return AIMessage(content=response_text)
            
            # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ì…ë ¥ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
            else:
                logger.info("ğŸ” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬")
                result = self._generate(input_data, **kwargs)
                return result.generations[0].message
                
        except Exception as e:
            logger.error(f"âŒ Gemini invoke ì‹¤íŒ¨: {e}")
            return AIMessage(content=f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


class ClaudeChatModel(BaseChatModel):
    """Anthropic Claude ëª¨ë¸ì„ ìœ„í•œ LangChain í˜¸í™˜ ë˜í¼"""
    
    def __init__(self, model_name: str, temperature: float = settings.TEMPERATURE, api_key: str = settings.ANTHROPIC_API_KEY):
        super().__init__()
        
        if not api_key:
            raise ModelError("ANTHROPIC_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # __dict__ë¥¼ ì‚¬ìš©í•˜ì—¬ í•„ë“œ ì„¤ì • (Pydantic ì œì•½ ìš°íšŒ)
        self.__dict__['model_name'] = model_name
        self.__dict__['temperature'] = temperature
        self.__dict__['api_key'] = api_key
        
        # Claude í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.__dict__['client'] = anthropic.Client(api_key=self.api_key)
    
    def _convert_messages_to_claude_format(self, messages: List[Any]) -> List[Dict[str, str]]:
        """LangChain ë©”ì‹œì§€ë¥¼ Claude í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        claude_messages = []
        
        for message in messages:
            if hasattr(message, 'content'):
                if isinstance(message, SystemMessage):
                    claude_messages.append({"role": "user", "content": f"System: {message.content}"})
                elif isinstance(message, HumanMessage):
                    claude_messages.append({"role": "user", "content": message.content})
                elif isinstance(message, AIMessage):
                    claude_messages.append({"role": "assistant", "content": message.content})
                else:
                    claude_messages.append({"role": "user", "content": str(message.content)})
            else:
                claude_messages.append({"role": "user", "content": str(message)})
        
        return claude_messages
    
    def _generate_response(self, messages: List[Any]) -> str:
        """Claude ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            claude_messages = self._convert_messages_to_claude_format(messages)
            
            response = self.client.messages.create(
                model=self.model_name,
                messages=claude_messages,
                max_tokens=ModelConfig.MAX_TOKENS,
                temperature=self.temperature
            )
            
            return response.content[0].text
            
        except Exception as e:
            logger.error(f"Claude ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise ModelError(f"Claude ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _generate(self, messages: List[Any], stop: Optional[List[str]] = None, run_manager: Optional[Any] = None, **kwargs) -> ChatResult:
        """LangChain í˜¸í™˜ ë©”ì‹œì§€ ìƒì„± ë©”ì„œë“œ"""
        try:
            response_text = self._generate_response(messages)
            
            return ChatResult(
                generations=[
                    ChatGeneration(
                        message=AIMessage(content=response_text)
                    )
                ]
            )
            
        except Exception as e:
            logger.error(f"Claude ë©”ì‹œì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    @property
    def _llm_type(self) -> str:
        return "claude"
    
    def invoke(self, messages: List[Any], config: Optional[Dict[str, Any]] = None, **kwargs) -> AIMessage:
        """LangChain 0.2.x í˜¸í™˜ invoke ë©”ì„œë“œ"""
        result = self._generate(messages, **kwargs)
        return result.generations[0].message

class ModelFactory:
    """ë‹¤ì–‘í•œ AI ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def get_supported_models() -> Dict[str, List[str]]:
        """ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return SUPPORTED_MODELS.copy()
    
    @staticmethod
    def is_model_supported(model_name: str) -> bool:
        """ëª¨ë¸ì´ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        for models in SUPPORTED_MODELS.values():
            if model_name in models:
                return True
        return False
    
    @staticmethod
    def create_llm(model_name: str) -> Optional[Union[ChatOpenAI, GeminiChatModel, ClaudeChatModel]]:
        """
        ëª¨ë¸ ì´ë¦„ì— ë”°ë¼ ì ì ˆí•œ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "gpt-4o", "gemini-2.0-flash", "claude-sonnet-4-0")
            
        Returns:
            LLM ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None (ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ì¸ ê²½ìš°)
            
        Raises:
            ModelError: ëª¨ë¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°
        """
        try:
            if not ModelFactory.is_model_supported(model_name):
                logger.error(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}")
                return None
            
            if model_name.startswith("gpt-"):
                return ModelFactory._create_openai_model(model_name)
            elif model_name.startswith("gemini-"):
                return ModelFactory._create_gemini_model(model_name)
            elif model_name.startswith("claude-"):
                return ModelFactory._create_claude_model(model_name)
            else:
                logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ í˜•ì‹: {model_name}")
                return None
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ ({model_name}): {e}")
            raise ModelError(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    @staticmethod
    def _create_openai_model(model_name: str) -> ChatOpenAI:
        """OpenAI ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        api_key = settings.OPENAI_API_KEY
        if not api_key:
            raise ModelError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return ChatOpenAI(
            model=model_name,
            temperature=settings.TEMPERATURE,
            api_key=api_key,
            max_tokens=ModelConfig.MAX_TOKENS,
            request_timeout=ModelConfig.REQUEST_TIMEOUT
        )
    
    @staticmethod
    def _create_gemini_model(model_name: str) -> GeminiChatModel:
        """Google Gemini ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        api_key = settings.GEMINI_API_KEY
        if not api_key:
            raise ModelError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì‹¤ì œ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
        actual_model_name = GEMINI_MODEL_MAPPING.get(model_name, model_name)
        
        return GeminiChatModel(
            model_name=actual_model_name,
            temperature=settings.TEMPERATURE,
            api_key=api_key
        )
    
    @staticmethod
    def _create_claude_model(model_name: str) -> ClaudeChatModel:
        """Anthropic Claude ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        api_key = settings.ANTHROPIC_API_KEY
        if not api_key:
            raise ModelError("ANTHROPIC_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì‹¤ì œ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
        actual_model_name = CLAUDE_MODEL_MAPPING.get(model_name, model_name)
        
        return ClaudeChatModel(
            model_name=actual_model_name,
            temperature=settings.TEMPERATURE,
            api_key=api_key
        )
